diff --git a/mm/memory.c b/mm/memory.c
index ab650c21b..320bd3336 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -2667,6 +2667,27 @@ void unmap_mapping_range(struct address_space *mapping,
 }
 EXPORT_SYMBOL(unmap_mapping_range);
 
+
+// TODO : This is anonymous page. Must be rmapped
+static bool swp_rmap_one(struct page *page, struct vm_area_struct *vma, 
+		        unsigned long addr, void *arg) 
+{
+        long *entry = arg;
+	pr_info("swptrace %s map %lu %p\n",
+			get_current()->comm, *entry, addr);
+	return true;	
+}
+
+void rmap_walk_swapanon(struct page *page, long entry) 
+{
+	// SWPTRACE
+	struct rmap_walk_control rwc = {
+		.rmap_one=swp_rmap_one,
+		.arg=(void*)&(entry),
+	};
+	rmap_walk(page, &rwc);
+}
+	
 /*
  * We enter with non-exclusive mmap_sem (to exclude vma changes,
  * but allow concurrent faults), and pte mapped but not yet locked.
@@ -2711,6 +2732,9 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 		goto out;
 	}
 
+        pr_info("swptrace %s fault %lu %p\n",
+			get_current()->comm, entry.val, vmf->address);
+ 
 
 	delayacct_set_flag(DELAYACCT_PF_SWAPIN);
 	page = lookup_swap_cache(entry, vma, vmf->address);
@@ -2846,6 +2870,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	}
 
 	swap_free(entry);
+
+
 	if (mem_cgroup_swap_full(page) ||
 	    (vma->vm_flags & VM_LOCKED) || PageMlocked(page))
 		try_to_free_swap(page);
@@ -2875,6 +2901,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 unlock:
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
 out:
+
+	rmap_walk_swapanon(page, entry.val);
 	return ret;
 out_nomap:
 	mem_cgroup_cancel_charge(page, memcg, false);
@@ -2887,6 +2915,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 		unlock_page(swapcache);
 		put_page(swapcache);
 	}
+	rmap_walk_swapanon(page, entry.val);
 	return ret;
 }
 
diff --git a/mm/migrate.c b/mm/migrate.c
index 663a54493..baeb2dd6b 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -296,8 +296,9 @@ void remove_migration_ptes(struct page *old, struct page *new, bool locked)
 
 	if (locked)
 		rmap_walk_locked(new, &rwc);
-	else
+	else 
 		rmap_walk(new, &rwc);
+		
 }
 
 /*
diff --git a/mm/page_io.c b/mm/page_io.c
index 2e8019d0e..d83602f61 100644
--- a/mm/page_io.c
+++ b/mm/page_io.c
@@ -26,6 +26,10 @@
 #include <linux/sched/task.h>
 #include <asm/pgtable.h>
 
+//TODO:swptrace 
+#include <linux/rmap.h>
+#include "internal.h" // vma_address
+
 static struct bio *get_swap_bio(gfp_t gfp_flags,
 				struct page *page, bio_end_io_t end_io)
 {
@@ -282,6 +286,25 @@ int __swap_writepage(struct page *page, struct writeback_control *wbc,
 	int ret;
 	struct swap_info_struct *sis = page_swap_info(page);
 
+	// TODO : throuth interverse the anon_vma_chain 
+	struct anon_vma * anon_vma; 
+	struct anon_vma_chain * avc;
+	pgoff_t pgoff_start, pgoff_end;
+
+	anon_vma = page_anon_vma(page);
+	
+
+	if (anon_vma) {
+		pgoff_start = page_to_pgoff(page);
+		pgoff_end = pgoff_start + hpage_nr_pages(page) - 1;	
+		anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff_start, pgoff_end) { 
+			struct vm_area_struct *vma = avc->vma;
+			unsigned long address = vma_address(page, vma);
+ 			pr_info("swptrace %s out %lu %p\n", get_current()->comm, page_private(page), address);
+		}
+	}
+	
+
 	VM_BUG_ON_PAGE(!PageSwapCache(page), page);
 	if (sis->flags & SWP_FS) {
 		struct kiocb kiocb;
@@ -359,6 +382,9 @@ int swap_readpage(struct page *page, bool synchronous)
 	VM_BUG_ON_PAGE(!PageSwapCache(page) && !synchronous, page);
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 	VM_BUG_ON_PAGE(PageUptodate(page), page);
+
+	pr_info("swptrace %s in %lu\n", get_current()->comm, page_private(page));
+
 	if (frontswap_load(page) == 0) {
 		SetPageUptodate(page);
 		unlock_page(page);
diff --git a/mm/rmap.c b/mm/rmap.c
index 76c8dfd3a..d6d26346d 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -1718,7 +1718,7 @@ bool try_to_unmap(struct page *page, enum ttu_flags flags)
 
 	if (flags & TTU_RMAP_LOCKED)
 		rmap_walk_locked(page, &rwc);
-	else
+	else 
 		rmap_walk(page, &rwc);
 
 	return !page_mapcount(page) ? true : false;
@@ -1828,8 +1828,11 @@ static void rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc,
 		if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
 			continue;
 
+		//if try_unmap_page_one() fails
 		if (!rwc->rmap_one(page, vma, address, rwc->arg))
 			break;
+
+		//if it is end of rwc or the page is done
 		if (rwc->done && rwc->done(page))
 			break;
 	}
diff --git a/mm/swap_state.c b/mm/swap_state.c
index 85245fdec..10b59c237 100644
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@ -560,6 +560,8 @@ struct page *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,
 		end_offset = si->max - 1;
 
 	blk_start_plug(&plug);
+	printk("swptrace %s ahead %lu %lu\n", get_current()->comm, swp_entry(swp_type(entry), start_offset), swp_entry(swp_type(entry), end_offset));
+
 	for (offset = start_offset; offset <= end_offset ; offset++) {
 		/* Ok, do the async read-ahead now */
 		page = __read_swap_cache_async(
@@ -576,6 +578,7 @@ struct page *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,
 		}
 		put_page(page);
 	}
+
 	blk_finish_plug(&plug);
 
 	lru_add_drain();	/* Push any new pages onto the LRU now */
@@ -729,6 +732,8 @@ static struct page *swap_vma_readahead(swp_entry_t fentry, gfp_t gfp_mask,
 		goto skip;
 
 	blk_start_plug(&plug);
+	printk("swptrace %s ahead %lu %lu\n", get_current()->comm, ra_info.ptes, (ra_info.ptes + ra_info.nr_pte));
+
 	for (i = 0, pte = ra_info.ptes; i < ra_info.nr_pte;
 	     i++, pte++) {
 		pentry = *pte;
@@ -751,6 +756,7 @@ static struct page *swap_vma_readahead(swp_entry_t fentry, gfp_t gfp_mask,
 			}
 		}
 		put_page(page);
+		
 	}
 	blk_finish_plug(&plug);
 	lru_add_drain();
