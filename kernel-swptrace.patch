diff --git a/mm/memory.c b/mm/memory.c
index ab650c21b..4b4433519 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -2667,6 +2667,17 @@ void unmap_mapping_range(struct address_space *mapping,
 }
 EXPORT_SYMBOL(unmap_mapping_range);
 
+// TODO : This is anonymous page. Must be rmapped
+static bool swp_rmap_one(struct page *page, struct vm_area_struct *vma, 
+		        unsigned long addr, void *arg) 
+{
+        long *entry = arg;
+	pr_info("swptrace %s map %lu %p\n",
+			get_current()->comm, *entry, page_to_virt(page));
+	return true;	
+}
+
+	
 /*
  * We enter with non-exclusive mmap_sem (to exclude vma changes,
  * but allow concurrent faults), and pte mapped but not yet locked.
@@ -2711,6 +2722,9 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 		goto out;
 	}
 
+        pr_info("swptrace %s fault %lu %p\n",
+			get_current()->comm, entry.val, vmf->address);
+ 
 
 	delayacct_set_flag(DELAYACCT_PF_SWAPIN);
 	page = lookup_swap_cache(entry, vma, vmf->address);
@@ -2846,6 +2860,14 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	}
 
 	swap_free(entry);
+
+	// SWPTRACE
+	struct rmap_walk_control rwc = {
+		.rmap_one=swp_rmap_one,
+		.arg=(void*)&(entry.val),
+	};
+	rmap_walk(page, &rwc);
+
 	if (mem_cgroup_swap_full(page) ||
 	    (vma->vm_flags & VM_LOCKED) || PageMlocked(page))
 		try_to_free_swap(page);
@@ -2875,6 +2897,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 unlock:
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
 out:
+
 	return ret;
 out_nomap:
 	mem_cgroup_cancel_charge(page, memcg, false);
@@ -2887,6 +2910,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 		unlock_page(swapcache);
 		put_page(swapcache);
 	}
+	
 	return ret;
 }
 
diff --git a/mm/migrate.c b/mm/migrate.c
index 663a54493..ddf0a835f 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -296,8 +296,11 @@ void remove_migration_ptes(struct page *old, struct page *new, bool locked)
 
 	if (locked)
 		rmap_walk_locked(new, &rwc);
-	else
+	else {
+		//printk("remove_migration_ptes/rmap.c\n");
 		rmap_walk(new, &rwc);
+	}
+		
 }
 
 /*
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index 9f61dfec6..12314196a 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -2298,6 +2298,7 @@ static int __writepage(struct page *page, struct writeback_control *wbc,
 {
 	struct address_space *mapping = data;
 	int ret = mapping->a_ops->writepage(page, wbc);
+	//pr_info("swptrace %s writepage pageout %p\n",get_current()->comm, page_address(page));
 	mapping_set_error(mapping, ret);
 	return ret;
 }
diff --git a/mm/page_idle.c b/mm/page_idle.c
index 0b39ec0c9..403886ee9 100644
--- a/mm/page_idle.c
+++ b/mm/page_idle.c
@@ -111,7 +111,7 @@ static void page_idle_clear_pte_refs(struct page *page)
 	need_lock = !PageAnon(page) || PageKsm(page);
 	if (need_lock && !trylock_page(page))
 		return;
-
+	//printk("page_idle_clear_pte_refs/page_idle.c\n");
 	rmap_walk(page, (struct rmap_walk_control *)&rwc);
 
 	if (need_lock)
diff --git a/mm/page_io.c b/mm/page_io.c
index 2e8019d0e..c71af9099 100644
--- a/mm/page_io.c
+++ b/mm/page_io.c
@@ -26,6 +26,10 @@
 #include <linux/sched/task.h>
 #include <asm/pgtable.h>
 
+//TODO:swptrace 
+#include <linux/rmap.h>
+#include "internal.h" // vma_address
+
 static struct bio *get_swap_bio(gfp_t gfp_flags,
 				struct page *page, bio_end_io_t end_io)
 {
@@ -282,6 +286,30 @@ int __swap_writepage(struct page *page, struct writeback_control *wbc,
 	int ret;
 	struct swap_info_struct *sis = page_swap_info(page);
 
+ 	// pr_info("|swptrace|%s|out|%lu|%p\n", get_current()->comm, page_private(page), page_to_virt(page));
+
+	// TODO : throuth interverse the anon_vma_chain 
+	// figure out whether there is a node with same anon_vma
+	// mm/rmap.c page_referenced_anon
+	// anon_vma : page->mapping 
+	struct anon_vma * anon_vma; 
+	struct anon_vma_chain * avc;
+	pgoff_t pgoff_start, pgoff_end;
+
+	anon_vma = page_anon_vma(page);
+	
+
+	if (anon_vma) {
+		pgoff_start = page_to_pgoff(page);
+		pgoff_end = pgoff_start + hpage_nr_pages(page) - 1;	
+		anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff_start, pgoff_end) { 
+			struct vm_area_struct *vma = avc->vma;
+			unsigned long address = vma_address(page, vma);
+ 			pr_info("swptrace %s out %lu %p\n", get_current()->comm, page_private(page), address);
+		}
+	}
+	
+
 	VM_BUG_ON_PAGE(!PageSwapCache(page), page);
 	if (sis->flags & SWP_FS) {
 		struct kiocb kiocb;
@@ -359,6 +387,9 @@ int swap_readpage(struct page *page, bool synchronous)
 	VM_BUG_ON_PAGE(!PageSwapCache(page) && !synchronous, page);
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 	VM_BUG_ON_PAGE(PageUptodate(page), page);
+
+	pr_info("swptrace %s in %lu\n", get_current()->comm, page_private(page));
+
 	if (frontswap_load(page) == 0) {
 		SetPageUptodate(page);
 		unlock_page(page);
diff --git a/mm/rmap.c b/mm/rmap.c
index 76c8dfd3a..b02bb0613 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -871,6 +871,7 @@ int page_referenced(struct page *page,
 		rwc.invalid_vma = invalid_page_referenced_vma;
 	}
 
+	//printk("page_referenced/rmap.c\n");
 	rmap_walk(page, &rwc);
 	*vm_flags = pra.vm_flags;
 
@@ -983,6 +984,7 @@ int page_mkclean(struct page *page)
 	if (!mapping)
 		return 0;
 
+	//printk("page_mkclean/rmap.c\n");
 	rmap_walk(page, &rwc);
 
 	return cleaned;
@@ -1718,8 +1720,10 @@ bool try_to_unmap(struct page *page, enum ttu_flags flags)
 
 	if (flags & TTU_RMAP_LOCKED)
 		rmap_walk_locked(page, &rwc);
-	else
+	else {
+		//printk("try_to_unmap/rmap.c\n");
 		rmap_walk(page, &rwc);
+	}
 
 	return !page_mapcount(page) ? true : false;
 }
@@ -1751,6 +1755,7 @@ void try_to_munlock(struct page *page)
 	VM_BUG_ON_PAGE(!PageLocked(page) || PageLRU(page), page);
 	VM_BUG_ON_PAGE(PageCompound(page) && PageDoubleMap(page), page);
 
+	//printk("try_to_munlock/rmap.c\n");
 	rmap_walk(page, &rwc);
 }
 
@@ -1818,6 +1823,7 @@ static void rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc,
 
 	pgoff_start = page_to_pgoff(page);
 	pgoff_end = pgoff_start + hpage_nr_pages(page) - 1;
+	long depth = 0; 
 	anon_vma_interval_tree_foreach(avc, &anon_vma->rb_root,
 			pgoff_start, pgoff_end) {
 		struct vm_area_struct *vma = avc->vma;
@@ -1828,8 +1834,12 @@ static void rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc,
 		if (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))
 			continue;
 
+		//if try_unmap_page_one() fails
 		if (!rwc->rmap_one(page, vma, address, rwc->arg))
 			break;
+		//printk("swptrace %d rmap_walk %lu %p\n", depth, page_private(page), address);
+		depth++;
+		//if it is end of rwc or the page is done
 		if (rwc->done && rwc->done(page))
 			break;
 	}
diff --git a/mm/swap_state.c b/mm/swap_state.c
index 85245fdec..392353797 100644
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@ -560,6 +560,8 @@ struct page *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,
 		end_offset = si->max - 1;
 
 	blk_start_plug(&plug);
+	printk("swptrace %s cluster_ahead_info %lu %lu\n", get_current()->comm, swp_entry(swp_type(entry), start_offset), swp_entry(swp_type(entry), end_offset));
+
 	for (offset = start_offset; offset <= end_offset ; offset++) {
 		/* Ok, do the async read-ahead now */
 		page = __read_swap_cache_async(
@@ -575,6 +577,7 @@ struct page *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,
 			}
 		}
 		put_page(page);
+	// printk("swptrace %s cluster_ahead %lu\n", get_current()->comm, page_private(page));
 	}
 	blk_finish_plug(&plug);
 
@@ -729,6 +732,8 @@ static struct page *swap_vma_readahead(swp_entry_t fentry, gfp_t gfp_mask,
 		goto skip;
 
 	blk_start_plug(&plug);
+	printk("swptrace %s vma_ahead_info %lu %lu\n", get_current()->comm, ra_info.win, ra_info.nr_pte, ra_info.ptes[0]);
+
 	for (i = 0, pte = ra_info.ptes; i < ra_info.nr_pte;
 	     i++, pte++) {
 		pentry = *pte;
@@ -751,6 +756,8 @@ static struct page *swap_vma_readahead(swp_entry_t fentry, gfp_t gfp_mask,
 			}
 		}
 		put_page(page);
+		printk("swptrace %s vma_ahead %lu %lu\n", get_current()->comm, page_private(page));
+		
 	}
 	blk_finish_plug(&plug);
 	lru_add_drain();
